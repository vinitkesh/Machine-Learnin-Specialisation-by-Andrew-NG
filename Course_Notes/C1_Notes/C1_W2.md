<a style="color:Orange;" ><h1 style="color:Orange;">
    Week 2: Regression with multiple input variables
</h1></a>  

## 0. Multiple Features
- Now we are estimating many things from the same set of input
- More variables
![Multiple variable model](image-18.png)

- For n features the model will look like this :
![n var func ](image-19.png)  
### Using vectors to represent variables and parameters :

![Multiple linear regression](image-20.png)
---


## 1. Vectorization part 1


![Alt text](image-21.png)

![Alt text](image-22.png)

## Using vectorization , the calculation is much faster:

![Vectorization](image-23.png)

```python
f = np.dot(w,x)+b
```



## 2. Vectorization part 2

### - Vectorization uses computesr specialised hardware to multiple and add very quickly. It uses parallel processing hardware.
### - Gradient Descent using vectorization :
(image-24.png)

## 3. Ungraded Lab 1: Optional lab: Python, NumPy and vectorization

### [Open lab Jupyter Notebook](https://github.com/vinitkesh/Machine-Learnin-Specialisation-by-Andrew-NG/blob/main/Labs/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb)



## 4. Gradient descent for multiple linear regression
## 5. Ungraded Lab 2: Optional Lab: Multiple linear regression
## 6. Feature scaling part 1
1. Scale of data withwhich it is plotted along x and y axis can slow down gradient descent.
   ![image](https://github.com/vinitkesh/Machine-Learnin-Specialisation-by-Andrew-NG/assets/139075087/ede3fbfa-f231-4a20-81bd-91b1d13e11c8)
   Rescaled:  
   ![image](https://github.com/vinitkesh/Machine-Learnin-Specialisation-by-Andrew-NG/assets/139075087/e9990214-fef6-4728-8719-6288f3e5d95c)

   

## 7. Feature scaling part 2
### Method of correcting scale of :
1. Dividing my max:
   ![image](https://github.com/vinitkesh/Machine-Learnin-Specialisation-by-Andrew-NG/assets/139075087/f22ac6b5-e3cd-45e9-9653-9eba0afb4311)
2. Mean Normalisation:
   Steps:
   1. find mean mu1
   2. Convert data as so : $$x1 = (x1 -mu1)/(maxRange-minRange)$$
  
3. Z-score Noramalisation
   - Converts into Z : Standard normal variable
   Steps:
    1. Find mean and SD
    2. Convert data : $$X = (X-mu)/(SD)$$
  
   ![image](https://github.com/vinitkesh/Machine-Learnin-Specialisation-by-Andrew-NG/assets/139075087/bc62e57c-c31e-45fa-bc97-74a074ea44b5)
<h2 style="color:orange;">RULE OF THUMB: <span style="color:pink;">Rescale to attain a range nearby -1 to 1</span> </h2>

    
## 8. Checking gradient descent for convergence
   - ### Helps chose right $\alpha$
   - ### Cost function should decrease for every iteration: 
   - ![Alt text](image-25.png)
   - #### Iteration need varies based on different scenarios - so this graph helps
   - 
## 9. Choosing the learning rate

1. #### Learning rate is too high:  
   ![Alt text](image-26.png)
2. #### When costs keep increaing:
   - Use minus sign with alpha
   - ![Alt text](image-27.png)
### Debugging step:
1. Take a small value of $\alpha$ and keep increasing it bit by bit
   ![Alt text](image-28.png)
2. 
## 10. Ungraded Lab 3: Optional Lab: Feature scaling and learning rate
## 11. Feature engineering
## 12. Polynomial regression
## 13. Ungraded Lab 4 : Optional lab: Feature engineering and Polynomial regression
## 14. Ungraded Lab 5: Optional lab: Linear regression with scikit-learn

---
